{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e07654c",
   "metadata": {},
   "source": [
    "# VAE Video Compression (Jupyter Project)\n",
    "Train a Variational Autoencoder (VAE) for video compression and compare against H.264/HEVC.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Installs/validates dependencies\n",
    "2. Loads your real dataset from `data/train` and `data/val`\n",
    "3. Defines a 3D-Conv VAE (temporal model)\n",
    "4. Trains multiple RD points (λ sweep)\n",
    "5. Evaluates VAE rate (KL→bpp) and distortion (PSNR/SSIM)\n",
    "6. Encodes baselines with ffmpeg (x264/x265), evaluates true bits\n",
    "7. Plots RD curves and computes category-wise BD-Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb4c63",
   "metadata": {},
   "source": [
    "## 0) Optional: Install/verify dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43649f9c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2881fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment to install/upgrade.\n",
    "# !python -m pip install --upgrade pip\n",
    "# CPU PyTorch (macOS typical)\n",
    "# !pip install torch torchvision torchaudio\n",
    "# Core deps\n",
    "# !pip install numpy opencv-python matplotlib tqdm scikit-image pyyaml scipy\n",
    "# Verify ffmpeg (install via brew/apt if missing)\n",
    "# !ffmpeg -version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceda38d",
   "metadata": {},
   "source": [
    "## 1) Imports & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53a92f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math, json, sys, time, shutil\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim_metric\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c5c534",
   "metadata": {},
   "source": [
    "## 2) Configure dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a687331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data/val/big_buck_bunny_240p_1mb.mp4...\n",
      "Downloading data/val/big_buck_bunny_240p_2mb.mp4...\n",
      "Downloading data/train/big_buck_bunny_240p_5mb.mp4...\n",
      "Downloading data/train/big_buck_bunny_240p_10mb.mp4...\n",
      "\n",
      "Train videos: 3 → ['.DS_Store', 'big_buck_bunny_240p_5mb.mp4', 'big_buck_bunny_240p_10mb.mp4']\n",
      "Val videos  : 3 → ['.DS_Store', 'big_buck_bunny_240p_2mb.mp4', 'big_buck_bunny_240p_1mb.mp4']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"data/train\", exist_ok=True)\n",
    "os.makedirs(\"data/val\", exist_ok=True)\n",
    "\n",
    "# Tiny MP4 clips for quick training/testing\n",
    "videos = {\n",
    "    \"data/val\": [\n",
    "        \"https://sample-videos.com/video321/mp4/240/big_buck_bunny_240p_1mb.mp4\",\n",
    "        \"https://sample-videos.com/video321/mp4/240/big_buck_bunny_240p_2mb.mp4\"\n",
    "    ],\n",
    "    \"data/train\": [\n",
    "        \"https://sample-videos.com/video321/mp4/240/big_buck_bunny_240p_5mb.mp4\",\n",
    "        \"https://sample-videos.com/video321/mp4/240/big_buck_bunny_240p_10mb.mp4\"\n",
    "       \n",
    "    ]\n",
    "}\n",
    "\n",
    "# Download function\n",
    "def download_file(url, dest_folder):\n",
    "    filename = os.path.join(dest_folder, os.path.basename(url.split(\"?\")[0]))\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    else:\n",
    "        print(f\"Already exists: {filename}\")\n",
    "\n",
    "# Fetch all videos\n",
    "for folder, urls in videos.items():\n",
    "    for url in urls:\n",
    "        download_file(url, folder)\n",
    "\n",
    "# Summary\n",
    "train_files = os.listdir(\"data/train\")\n",
    "val_files = os.listdir(\"data/val\")\n",
    "print(f\"\\nTrain videos: {len(train_files)} → {train_files}\")\n",
    "print(f\"Val videos  : {len(val_files)} → {val_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "139280f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix A: helper to make time dims match (center-crop longer tensor)\n",
    "import torch.nn.functional as F\n",
    "import math, numpy as np\n",
    "\n",
    "def align_time(a, b):\n",
    "    \"\"\"\n",
    "    a,b: (B,C,T,H,W). Center-crop longer one along T so both match.\n",
    "    Returns tensors with identical T.\n",
    "    \"\"\"\n",
    "    Ta, Tb = a.shape[2], b.shape[2]\n",
    "    if Ta == Tb:\n",
    "        return a, b\n",
    "    T = min(Ta, Tb)\n",
    "    def crop_t(x, T):\n",
    "        start = (x.shape[2] - T) // 2\n",
    "        return x[:, :, start:start+T]\n",
    "    if Ta != T: a = crop_t(a, T)\n",
    "    if Tb != T: b = crop_t(b, T)\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccc83519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader workers disabled: {'num_workers': 0, 'persistent_workers': False, 'pin_memory': False}\n"
     ]
    }
   ],
   "source": [
    "# --- STABILITY PATCH for macOS/Jupyter DataLoader worker crashes ---\n",
    "\n",
    "import os, cv2, torch, torch.multiprocessing as mp\n",
    "\n",
    "# OpenCV + multiprocessing can crash on macOS; keep things single-threaded.\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "# Make sure PyTorch uses a safe start method in notebooks\n",
    "try:\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "except RuntimeError:\n",
    "    pass  # already set\n",
    "\n",
    "# Helper flags so we don't forget\n",
    "DATALOADER_KW = dict(num_workers=0, persistent_workers=False, pin_memory=False)\n",
    "print(\"DataLoader workers disabled:\", DATALOADER_KW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "30ce82ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, **DATALOADER_KW)\n\u001b[32m      4\u001b[39m val_ld   = DataLoader(val_ds,   batch_size=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, **DATALOADER_KW)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, **DATALOADER_KW)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=1, shuffle=False, **DATALOADER_KW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063c8987",
   "metadata": {},
   "source": [
    "## 3) Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a209f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix B: robust __getitem__ that guarantees exactly `self.frames` frames\n",
    "import cv2, torch, numpy as np\n",
    "\n",
    "class VideoFolderDataset(VideoFolderDataset):  # subclass/override\n",
    "    def __getitem__(self, idx):\n",
    "        vi, s = self.index[idx]\n",
    "        path = self.items[vi]\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, s)\n",
    "        for _ in range(self.frames):\n",
    "            ok, f = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "            h, w = f.shape[:2]\n",
    "            Ht, Wt = self.size\n",
    "            scale = max(Ht/h, Wt/w)\n",
    "            nh, nw = int(round(h*scale)), int(round(w*scale))\n",
    "            f = cv2.resize(f, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "            y0 = (nh - Ht)//2; x0 = (nw - Wt)//2\n",
    "            f = f[y0:y0+Ht, x0:x0+Wt]\n",
    "            frames.append(f)\n",
    "        cap.release()\n",
    "\n",
    "        # Ensure at least 1 frame\n",
    "        if len(frames) == 0:\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            ok, f = cap.read()\n",
    "            cap.release()\n",
    "            if not ok:\n",
    "                raise RuntimeError(f\"Could not read any frame from {path}\")\n",
    "            f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "            h, w = f.shape[:2]\n",
    "            Ht, Wt = self.size\n",
    "            scale = max(Ht/h, Wt/w)\n",
    "            nh, nw = int(round(h*scale)), int(round(w*scale))\n",
    "            f = cv2.resize(f, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "            y0 = (nh - Ht)//2; x0 = (nw - Wt)//2\n",
    "            f = f[y0:y0+Ht, x0:x0+Wt]\n",
    "            frames = [f]\n",
    "\n",
    "        # Pad or crop to exactly self.frames\n",
    "        while len(frames) < self.frames:\n",
    "            frames.append(frames[-1])\n",
    "        if len(frames) > self.frames:\n",
    "            start = (len(frames) - self.frames) // 2\n",
    "            frames = frames[start:start+self.frames]\n",
    "\n",
    "        arr = np.stack(frames, axis=0)  # T,H,W,3\n",
    "        arr = torch.from_numpy(arr).permute(3,0,1,2).float()/255.0  # C,T,H,W\n",
    "        return arr, os.path.basename(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e87a10",
   "metadata": {},
   "source": [
    "## 4) VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3dfa98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix C: patched train/eval that call align_time() before loss/metrics\n",
    "import torch, os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _psnr(x, y, eps=1e-8):\n",
    "    mse = F.mse_loss(x, y).item()\n",
    "    return 10.0 * math.log10(1.0 / (mse + eps))\n",
    "\n",
    "def train_vae(lambda_mse=0.01, beta_kl=1.0, frames=4, size=(128,128), window_step=4,\n",
    "              epochs=3, batch_size=1, lr=1e-4, save_dir='runs/vae_quickfix',\n",
    "              device=('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    # reuse your already-defined classes\n",
    "    model = VAEVideo().to(device)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_ds = VideoFolderDataset(DATA_TRAIN, frames=frames, size=size, window_step=window_step)\n",
    "    val_ds   = VideoFolderDataset(DATA_VAL,   frames=frames, size=size, window_step=window_step)\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_ld   = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best_psnr = -1e9\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for x, _ in train_ld:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            xrec, kl = model(x)\n",
    "            x_adj, xrec_adj = align_time(x, xrec)              # <<< fix\n",
    "            loss_mse = F.mse_loss(x_adj, xrec_adj)\n",
    "            loss = lambda_mse * loss_mse + beta_kl * kl\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        # quick val\n",
    "        model.eval(); vs=[]\n",
    "        with torch.no_grad():\n",
    "            for x, _ in val_ld:\n",
    "                x = x.to(device)\n",
    "                xrec, _ = model(x)\n",
    "                x_adj, xrec_adj = align_time(x, xrec)          # <<< fix\n",
    "                vs.append(_psnr(x_adj, xrec_adj))\n",
    "        cur = float(np.mean(vs)) if vs else -1e9\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'last.pt'))\n",
    "        if cur > best_psnr:\n",
    "            best_psnr = cur\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best.pt'))\n",
    "        print(f\"Epoch {epoch}/{epochs}  Val PSNR: {cur:.2f} (best {best_psnr:.2f})\")\n",
    "\n",
    "    return os.path.join(save_dir, 'best.pt')\n",
    "\n",
    "def eval_vae(ckpt_path, frames=4, size=(128,128), window_step=4,\n",
    "             device=('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    # reuse your utils from the notebook\n",
    "    ds = VideoFolderDataset(DATA_VAL, frames=frames, size=size, window_step=window_step)\n",
    "    ld = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = VAEVideo().to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "    points = []\n",
    "    with torch.no_grad():\n",
    "        for x, name in ld:\n",
    "            x = x.to(device)\n",
    "            xrec, kl = model(x)\n",
    "            x_adj, xrec_adj = align_time(x, xrec)              # <<< fix\n",
    "            m = {\n",
    "                \"name\": name[0].rsplit('.',1)[0],\n",
    "                \"psnr\": float(_psnr(x_adj, xrec_adj)),\n",
    "                \"ssim\": float(ssim_torch(x_adj, xrec_adj)),\n",
    "                \"rate_bpp\": float(kl_bits_per_pixel(kl, x_adj.shape)),  # use adjusted T\n",
    "            }\n",
    "            points.append(m)\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8670d894",
   "metadata": {},
   "source": [
    "## 5) Metrics & utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caecd46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(x, y): return F.mse_loss(x, y)\n",
    "\n",
    "def psnr(x, y, eps=1e-8):\n",
    "    mse = mse_loss(x, y).item()\n",
    "    return 10.0 * math.log10(1.0 / (mse + eps))\n",
    "\n",
    "def ssim_torch(x, y):\n",
    "    w = torch.tensor([0.299, 0.587, 0.114], device=x.device).view(1,3,1,1,1)\n",
    "    xg = (x*w).sum(1, keepdim=True)\n",
    "    yg = (y*w).sum(1, keepdim=True)\n",
    "    C1 = 0.01**2; C2 = 0.03**2\n",
    "    mu_x = F.avg_pool3d(xg, kernel_size=(x.shape[2],11,11), stride=1, padding=(0,5,5))\n",
    "    mu_y = F.avg_pool3d(yg, kernel_size=(y.shape[2],11,11), stride=1, padding=(0,5,5))\n",
    "    sigma_x  = F.avg_pool3d(xg*xg, kernel_size=(x.shape[2],11,11), stride=1, padding=(0,5,5)) - mu_x**2\n",
    "    sigma_y  = F.avg_pool3d(yg*yg, kernel_size=(y.shape[2],11,11), stride=1, padding=(0,5,5)) - mu_y**2\n",
    "    sigma_xy = F.avg_pool3d(xg*yg, kernel_size=(x.shape[2],11,11), stride=1, padding=(0,5,5)) - mu_x*mu_y\n",
    "    ssim_map = ((2*mu_x*mu_y + C1)*(2*sigma_xy + C2))/((mu_x**2 + mu_y**2 + C1)*(sigma_x + sigma_y + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "def kl_bits_per_pixel(kl_scalar, x_shape):\n",
    "    B,C,T,H,W = x_shape\n",
    "    n_pix = B*T*H*W\n",
    "    bits = float(kl_scalar) / math.log(2)\n",
    "    return bits / (n_pix + 1e-12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04431b68",
   "metadata": {},
   "source": [
    "## 6) Train (single λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c6e6d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(lambda_mse=0.01, beta_kl=1.0, frames=8, size=(256,256), window_step=8,\n",
    "             epochs=5, batch_size=2, lr=1e-4, save_dir='runs/vae', device=device):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    train_ds = VideoFolderDataset(DATA_TRAIN, frames=frames, size=size, window_step=window_step)\n",
    "    val_ds   = VideoFolderDataset(DATA_VAL,   frames=frames, size=size, window_step=window_step)\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_ld   = DataLoader(val_ds, batch_size=1, shuffle=False)\n",
    "    model = VAEVideo().to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    best_psnr = -1e9\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train(); pbar = tqdm(train_ld, desc=f'Epoch {epoch}/{epochs}')\n",
    "        for x,_ in pbar:\n",
    "            x=x.to(device); xrec, kl = model(x)\n",
    "            loss_mse = mse_loss(x,xrec); loss = lambda_mse*loss_mse + beta_kl*kl\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            pbar.set_postfix(loss=float(loss.item()), mse=float(loss_mse.item()), kl=float(kl.item()))\n",
    "        model.eval(); psnrs=[]\n",
    "        with torch.no_grad():\n",
    "            for x,_ in DataLoader(val_ds, batch_size=1, shuffle=False):\n",
    "                x=x.to(device); xrec,_=model(x); psnrs.append(psnr(x,xrec))\n",
    "        cur=float(np.mean(psnrs));\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir,'last.pt'))\n",
    "        if cur>best_psnr:\n",
    "            best_psnr=cur; torch.save(model.state_dict(), os.path.join(save_dir,'best.pt'))\n",
    "        print(f'Val PSNR: {cur:.2f} (best {best_psnr:.2f})')\n",
    "    return os.path.join(save_dir,'best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f2c3eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ---- SPEED MODE PATCH ----\n",
    "import os, math, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Pick the fastest device available\n",
    "device = (\n",
    "    \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available()\n",
    "    else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Reuse your helpers if already defined: align_time, ssim_torch, kl_bits_per_pixel, VideoFolderDataset, VAEVideo\n",
    "\n",
    "def _psnr(x, y, eps=1e-8):\n",
    "    mse = F.mse_loss(x, y).item()\n",
    "    return 10.0 * math.log10(1.0 / (mse + eps))\n",
    "\n",
    "def train_vae_fast(\n",
    "    lambda_mse=0.01,\n",
    "    beta_kl=1.0,\n",
    "    frames=4,\n",
    "    size=(96,96),\n",
    "    window_step=12,\n",
    "    epochs=3,\n",
    "    batch_size=1,\n",
    "    lr=1e-4,\n",
    "    channels=32,        # smaller model width\n",
    "    latent_dim=96,      # smaller latent\n",
    "    val_max_batches=4,  # validate on only a few batches\n",
    "    num_workers=2,\n",
    "    save_dir=\"runs/fast\"\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # datasets / loaders\n",
    "    train_ds = VideoFolderDataset(DATA_TRAIN, frames=frames, size=size, window_step=window_step)\n",
    "    val_ds   = VideoFolderDataset(DATA_VAL,   frames=frames, size=size, window_step=window_step)\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=(device!=\"cpu\"))\n",
    "    val_ld   = DataLoader(val_ds,   batch_size=1, shuffle=False, num_workers=max(1,num_workers-1), pin_memory=(device!=\"cpu\"))\n",
    "\n",
    "    # smaller model\n",
    "    model = VAEVideo(in_ch=3, latent_dim=latent_dim, base=channels, groups=8).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    best_psnr = -1e9\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for x, _ in train_ld:\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            xrec, kl = model(x)\n",
    "            # align_time() should already be defined in your notebook; if not, just remove this line\n",
    "            x_adj, xrec_adj = align_time(x, xrec) if 'align_time' in globals() else (x, xrec)\n",
    "            loss_mse = F.mse_loss(x_adj, xrec_adj)\n",
    "            loss = lambda_mse * loss_mse + beta_kl * kl\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # quick/cheap validation\n",
    "        model.eval()\n",
    "        vs = []\n",
    "        with torch.no_grad():\n",
    "            for i, (x, _) in enumerate(val_ld):\n",
    "                if i >= val_max_batches: break\n",
    "                x = x.to(device)\n",
    "                xrec, _ = model(x)\n",
    "                x_adj, xrec_adj = align_time(x, xrec) if 'align_time' in globals() else (x, xrec)\n",
    "                vs.append(_psnr(x_adj, xrec_adj))\n",
    "        cur = float(np.mean(vs)) if vs else -1e9\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"last.pt\"))\n",
    "        if cur > best_psnr:\n",
    "            best_psnr = cur\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"best.pt\"))\n",
    "        print(f\"[FAST] Epoch {epoch}/{epochs}  Val PSNR (subset): {cur:.2f}  (best {best_psnr:.2f})\")\n",
    "\n",
    "    return os.path.join(save_dir, \"best.pt\")\n",
    "\n",
    "def eval_vae_fast(\n",
    "    ckpt_path,\n",
    "    frames=4,\n",
    "    size=(96,96),\n",
    "    window_step=12,\n",
    "    num_workers=1\n",
    "):\n",
    "    ds = VideoFolderDataset(DATA_VAL, frames=frames, size=size, window_step=window_step)\n",
    "    ld = DataLoader(ds, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = VAEVideo(in_ch=3, latent_dim=96, base=32, groups=8).to(device)  # must match train_vae_fast\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    points = []\n",
    "    with torch.no_grad():\n",
    "        for x, name in ld:\n",
    "            x = x.to(device)\n",
    "            xrec, kl = model(x)\n",
    "            x_adj, xrec_adj = align_time(x, xrec) if 'align_time' in globals() else (x, xrec)\n",
    "            points.append({\n",
    "                \"name\": name[0].rsplit('.',1)[0],\n",
    "                \"psnr\": float(_psnr(x_adj, xrec_adj)),\n",
    "                \"ssim\": float(ssim_torch(x_adj, xrec_adj)) if 'ssim_torch' in globals() else None,\n",
    "                \"rate_bpp\": float(kl_bits_per_pixel(kl, x_adj.shape)) if 'kl_bits_per_pixel' in globals() else None,\n",
    "            })\n",
    "    return points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a06c4",
   "metadata": {},
   "source": [
    "## 7) Train an RD sweep (λ values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96d7acc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FAST mode: λ=0.003 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/mawahid/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"/Users/mawahid/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/Users/mawahid/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"/Users/mawahid/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "        self = reduction.pickle.load(from_parent)self = reduction.pickle.load(from_parent)\n",
      "\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "AttributeErrorAttributeError: : Can't get attribute 'VideoFolderDataset' on <module '__main__' (built-in)>Can't get attribute 'VideoFolderDataset' on <module '__main__' (built-in)>\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 62179, 62180) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     data = \u001b[38;5;28mself\u001b[39m._data_queue.get(timeout=timeout)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    112\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = wait([\u001b[38;5;28mself\u001b[39m], timeout)\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/multiprocessing/connection.py:948\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m    947\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m     ready = selector.select(timeout)\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28mself\u001b[39m._selector.poll(timeout)\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[39m, in \u001b[36m_set_SIGCHLD_handler.<locals>.handler\u001b[39m\u001b[34m(signum, frame)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandler\u001b[39m(signum, frame):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     _error_if_any_worker_fails()\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid 62179) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lam \u001b[38;5;129;01min\u001b[39;00m lambdas:\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== FAST mode: λ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     ckpt = train_vae_fast(\n\u001b[32m      7\u001b[39m         lambda_mse=lam,\n\u001b[32m      8\u001b[39m         epochs=\u001b[32m3\u001b[39m,\n\u001b[32m      9\u001b[39m         frames=\u001b[32m4\u001b[39m,\n\u001b[32m     10\u001b[39m         size=(\u001b[32m96\u001b[39m,\u001b[32m96\u001b[39m),\n\u001b[32m     11\u001b[39m         window_step=\u001b[32m12\u001b[39m,\n\u001b[32m     12\u001b[39m         channels=\u001b[32m32\u001b[39m,\n\u001b[32m     13\u001b[39m         latent_dim=\u001b[32m96\u001b[39m,\n\u001b[32m     14\u001b[39m         save_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns/fast_lam\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     16\u001b[39m     checkpoints[lam] = ckpt\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# quick eval\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtrain_vae_fast\u001b[39m\u001b[34m(lambda_mse, beta_kl, frames, size, window_step, epochs, batch_size, lr, channels, latent_dim, val_max_batches, num_workers, save_dir)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     48\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x, _ \u001b[38;5;129;01min\u001b[39;00m train_ld:\n\u001b[32m     50\u001b[39m         x = x.to(device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     51\u001b[39m         xrec, kl = model(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28mself\u001b[39m._next_data()\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28mself\u001b[39m._get_data()\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1454\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m         success, data = \u001b[38;5;28mself\u001b[39m._try_get_data()\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vae_video_compression/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1298\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1297\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 62179, 62180) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# λ sweep in speed mode (much faster)\n",
    "lambdas = [0.003, 0.01, 0.03]\n",
    "checkpoints = {}\n",
    "for lam in lambdas:\n",
    "    print(f\"\\n=== FAST mode: λ={lam} ===\")\n",
    "    ckpt = train_vae_fast(\n",
    "        lambda_mse=lam,\n",
    "        epochs=3,\n",
    "        frames=4,\n",
    "        size=(96,96),\n",
    "        window_step=12,\n",
    "        channels=32,\n",
    "        latent_dim=96,\n",
    "        save_dir=f\"runs/fast_lam{lam}\"\n",
    "    )\n",
    "    checkpoints[lam] = ckpt\n",
    "\n",
    "# quick eval\n",
    "vae_points = []\n",
    "for lam, ckpt in checkpoints.items():\n",
    "    vae_points.extend(eval_vae_fast(ckpt, frames=4, size=(96,96), window_step=12))\n",
    "print(\"VAE points:\", len(vae_points))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d25e6e",
   "metadata": {},
   "source": [
    "## 8) Evaluate VAE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37315639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_vae(ckpt_path, frames=8, size=(256,256), window_step=8, device=device):\n",
    "    ds = VideoFolderDataset(DATA_VAL, frames=frames, size=size, window_step=window_step)\n",
    "    ld = DataLoader(ds, batch_size=1, shuffle=False)\n",
    "    model = VAEVideo().to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval(); points=[]\n",
    "    with torch.no_grad():\n",
    "        for x,name in tqdm(ld, desc='Eval VAE'):\n",
    "            x=x.to(device); xrec,kl=model(x)\n",
    "            points.append({'name': name[0].rsplit('.',1)[0], 'psnr': float(psnr(x,xrec)), 'ssim': float(ssim_torch(x,xrec)), 'rate_bpp': float(kl_bits_per_pixel(kl, x.shape))})\n",
    "    return points\n",
    "\n",
    "vae_points=[]\n",
    "for lam,ckpt in checkpoints.items():\n",
    "    vae_points.extend(eval_vae(ckpt, frames=4, size=(128,128), window_step=4))\n",
    "len(vae_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f0e90",
   "metadata": {},
   "source": [
    "## 9) Baselines — ffmpeg encodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb763ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def encode_dir(codec_label, crfs, out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    videos = sorted(glob.glob(os.path.join(DATA_VAL, '*')))\n",
    "    for v in videos:\n",
    "        base = os.path.splitext(os.path.basename(v))[0]\n",
    "        for crf in crfs:\n",
    "            if codec_label=='x264':\n",
    "                cmd=['ffmpeg','-y','-i',v,'-c:v','libx264','-preset','medium','-crf',str(crf),'-pix_fmt','yuv420p', os.path.join(out_dir,f'{base}_crf{crf}.mp4')]\n",
    "            else:\n",
    "                cmd=['ffmpeg','-y','-i',v,'-c:v','libx265','-preset','medium','-crf',str(crf),'-pix_fmt','yuv420p', os.path.join(out_dir,f'{base}_crf{crf}.mp4')]\n",
    "            print('Running:', ' '.join(cmd)); subprocess.run(cmd, check=True)\n",
    "\n",
    "encode_dir('x264', [18,22,26,30,34], 'outputs/x264')\n",
    "encode_dir('x265', [20,24,28,32,36], 'outputs/x265')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0096c2f0",
   "metadata": {},
   "source": [
    "## 10) Evaluate baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5eca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_frames(path):\n",
    "    cap=cv2.VideoCapture(path); frames=[]; ok=True\n",
    "    while ok:\n",
    "        ok,f=cap.read();\n",
    "        if ok: frames.append(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))\n",
    "    cap.release(); return frames\n",
    "\n",
    "def eval_ffmpeg(enc_root):\n",
    "    points=[]\n",
    "    ref_videos = sorted(glob.glob(os.path.join(DATA_VAL, '*')))\n",
    "    for ref in ref_videos:\n",
    "        base=os.path.splitext(os.path.basename(ref))[0]\n",
    "        cands=sorted(glob.glob(os.path.join(enc_root, f'{base}*.*')))\n",
    "        if not cands: continue\n",
    "        ref_fs=decode_frames(ref); H,W=ref_fs[0].shape[:2]\n",
    "        for enc in cands:\n",
    "            enc_fs=decode_frames(enc); n=min(len(ref_fs), len(enc_fs))\n",
    "            ps=[]\n",
    "            for i in range(n):\n",
    "                x=ref_fs[i].astype(np.float32)/255.0; y=enc_fs[i].astype(np.float32)/255.0\n",
    "                mse=np.mean((x-y)**2)+1e-8; ps.append(10*np.log10(1.0/mse))\n",
    "            bits=os.path.getsize(enc)*8; bpp=bits/(H*W*n)\n",
    "            points.append({'name': base, 'file': os.path.basename(enc), 'psnr': float(np.mean(ps)), 'rate_bpp': float(bpp)})\n",
    "    return points\n",
    "\n",
    "x264_points=eval_ffmpeg('outputs/x264'); x265_points=eval_ffmpeg('outputs/x265')\n",
    "len(x264_points), len(x265_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35f405",
   "metadata": {},
   "source": [
    "## 11) Plot RD curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab670a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rd(vae_pts, x264_pts, x265_pts):\n",
    "    def _xy(points):\n",
    "        xs=[p['rate_bpp'] for p in points]; ys=[p['psnr'] for p in points]\n",
    "        order=np.argsort(xs); return np.array(xs)[order], np.array(ys)[order]\n",
    "    plt.figure()\n",
    "    vx,vy=_xy(vae_pts); plt.scatter(vx,vy,label='VAE (KL→bpp)', marker='o')\n",
    "    x4,y4=_xy(x264_pts); plt.plot(x4,y4,'-o',label='H.264 (x264)')\n",
    "    x5,y5=_xy(x265_pts); plt.plot(x5,y5,'-o',label='H.265 (x265)')\n",
    "    plt.xlabel('Rate (bits/pixel)'); plt.ylabel('PSNR (dB)'); plt.title('Rate–Distortion')\n",
    "    plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "plot_rd(vae_points, x264_points, x265_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7aca23",
   "metadata": {},
   "source": [
    "## 12) Category-wise BD-Rate (requires scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def bdrate(rate1, psnr1, rate2, psnr2):\n",
    "    r1, p1 = np.array(rate1, float), np.array(psnr1, float)\n",
    "    r2, p2 = np.array(rate2, float), np.array(psnr2, float)\n",
    "    m1=(r1>0)&np.isfinite(r1)&np.isfinite(p1); m2=(r2>0)&np.isfinite(r2)&np.isfinite(p2)\n",
    "    r1,p1=r1[m1],p1[m1]; r2,p2=r2[m2],p2[m2]\n",
    "    if len(r1)<2 or len(r2)<2: return None\n",
    "    pmin,pmax=max(p1.min(),p2.min()), min(p1.max(),p2.max())\n",
    "    if not np.isfinite(pmin) or not np.isfinite(pmax) or pmax<=pmin: return None\n",
    "    f1=interp1d(p1, np.log(r1), kind='linear', fill_value='extrapolate')\n",
    "    f2=interp1d(p2, np.log(r2), kind='linear', fill_value='extrapolate')\n",
    "    xs=np.linspace(pmin,pmax,200); r1i=np.exp(f1(xs)); r2i=np.exp(f2(xs))\n",
    "    dr=(np.trapz(r1i,xs)/(pmax-pmin))/(np.trapz(r2i,xs)/(pmax-pmin)) - 1.0\n",
    "    return float(dr*100.0)\n",
    "\n",
    "# Edit your categories mapping below\n",
    "categories={}\n",
    "\n",
    "vae_by=defaultdict(list)\n",
    "for p in vae_points: vae_by[p['name']].append(p)\n",
    "\n",
    "x264_by=defaultdict(list)\n",
    "for p in x264_points: x264_by[p['name']].append(p)\n",
    "\n",
    "x265_by=defaultdict(list)\n",
    "for p in x265_points: x265_by[p['name']].append(p)\n",
    "\n",
    "report={}\n",
    "for name, vpts in vae_by.items():\n",
    "    cat=categories.get(name,'unknown')\n",
    "    if x264_by.get(name):\n",
    "        bd1=bdrate([p['rate_bpp'] for p in vpts],[p['psnr'] for p in vpts], [p['rate_bpp'] for p in x264_by[name]], [p['psnr'] for p in x264_by[name]])\n",
    "        report.setdefault(cat,{}).setdefault('BD-Rate vs H.264 (%)',[]).append(bd1)\n",
    "    if x265_by.get(name):\n",
    "        bd2=bdrate([p['rate_bpp'] for p in vpts],[p['psnr'] for p in vpts], [p['rate_bpp'] for p in x265_by[name]], [p['psnr'] for p in x265_by[name]])\n",
    "        report.setdefault(cat,{}).setdefault('BD-Rate vs H.265 (%)',[]).append(bd2)\n",
    "\n",
    "# average per category\n",
    "for cat,d in report.items():\n",
    "    for k,arr in list(d.items()):\n",
    "        arr=[a for a in arr if a is not None]\n",
    "        d[k]=float(np.mean(arr)) if arr else None\n",
    "\n",
    "print(json.dumps(report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f59305",
   "metadata": {},
   "source": [
    "## 13) Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74983734",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('runs/notebook', exist_ok=True)\n",
    "import json\n",
    "with open('runs/notebook/vae_points.json','w') as f: json.dump(vae_points,f,indent=2)\n",
    "with open('runs/notebook/x264_points.json','w') as f: json.dump(x264_points,f,indent=2)\n",
    "with open('runs/notebook/x265_points.json','w') as f: json.dump(x265_points,f,indent=2)\n",
    "print('Saved to runs/notebook/*.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
